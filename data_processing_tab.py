# data_processing_tab.py (ä¿®æ­£ç‰ˆ - æ§‹æ–‡ã‚¨ãƒ©ãƒ¼è§£æ±º)

import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
import streamlit as st
import pandas as pd
import numpy as np
import time
import os
import tempfile
import gc
import psutil
import logging
import traceback  # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 

logger = logging.getLogger(__name__)

from integrated_preprocessing import (
    integrated_preprocess_data, calculate_file_hash, efficient_duplicate_check
)
from loader import load_files
from forecast import generate_filtered_summaries
from utils import initialize_all_mappings, create_dept_mapping_table

EXCEL_USE_COLUMNS = [
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰", "è¨ºç™‚ç§‘å", "æ—¥ä»˜", "åœ¨é™¢æ‚£è€…æ•°",
    "å…¥é™¢æ‚£è€…æ•°", "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°", "é€€é™¢æ‚£è€…æ•°", "æ­»äº¡æ‚£è€…æ•°"
]
EXCEL_DTYPES = {
    "ç—…æ£Ÿã‚³ãƒ¼ãƒ‰": str, "è¨ºç™‚ç§‘å": str, "åœ¨é™¢æ‚£è€…æ•°": float,
    "å…¥é™¢æ‚£è€…æ•°": float, "ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°": float, "é€€é™¢æ‚£è€…æ•°": float, "æ­»äº¡æ‚£è€…æ•°": float
}

def log_memory_usage():
    try:
        process = psutil.Process()
        mem_info = process.memory_info()
        return {
            'process_mb': mem_info.rss / (1024 * 1024), 
            'process_percent': process.memory_percent(),
            'system_percent': psutil.virtual_memory().percent, 
            'available_mb': psutil.virtual_memory().available / (1024 * 1024)
        }
    except Exception as e:
        logger.error(f"ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

def perform_cleanup(deep=False):
    if deep and 'df' in st.session_state and st.session_state.df is not None:
        if 'filtered_results' in st.session_state and st.session_state.get('filtered_results') != st.session_state.get('all_results'):
            st.session_state.filtered_results = None
        if 'forecast_model_results' in st.session_state:
            st.session_state.forecast_model_results = None
    try:
        temp_dir_root = tempfile.gettempdir()
        app_temp_files_pattern = os.path.join(temp_dir_root, "integrated_dashboard_temp_*")
        import glob, shutil
        for temp_file_path in glob.glob(app_temp_files_pattern):
            try:
                if os.path.isfile(temp_file_path): 
                    os.unlink(temp_file_path)
                elif os.path.isdir(temp_file_path): 
                    shutil.rmtree(temp_file_path, ignore_errors=True)
            except Exception as e_file_del: 
                logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_file_del}")
    except Exception as e_temp_clean: 
        logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_temp_clean}")
    gc.collect()
    time.sleep(0.1)
    gc.collect()

def get_app_data_dir():
    base_temp_dir = tempfile.gettempdir()
    app_data_dir = os.path.join(base_temp_dir, "integrated_dashboard_data")
    if not os.path.exists(app_data_dir):
        try: 
            os.makedirs(app_data_dir, exist_ok=True)
        except OSError as e: 
            st.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—: {app_data_dir}\n{e}")
            return None
    return app_data_dir

def get_base_file_info(app_data_dir):
    if app_data_dir is None: 
        return None
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    if os.path.exists(info_path):
        try:
            import json
            with open(info_path, 'r', encoding='utf-8') as f: 
                return json.load(f)
        except Exception as e: 
            logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None
    return None

def save_base_file_info(app_data_dir, file_name, file_size, file_hash):
    if app_data_dir is None: 
        return
    info_path = os.path.join(app_data_dir, "base_file_info.json")
    info = {"file_name": file_name, "file_size": file_size, "file_hash": file_hash}
    try:
        import json
        with open(info_path, 'w', encoding='utf-8') as f: 
            json.dump(info, f, ensure_ascii=False, indent=2)
    except Exception as e: 
        logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

def debug_target_file_processing(target_data, search_keywords=['å…¨ä½“', 'ç—…é™¢å…¨ä½“', 'ç—…é™¢']):
    debug_info = {
        'file_loaded': target_data is not None, 
        'columns': [], 
        'shape': (0,0), 
        'search_results': {}, 
        'sample_data': None
    }
    if target_data is not None and not target_data.empty:
        debug_info['columns'] = list(target_data.columns)
        debug_info['shape'] = target_data.shape
        debug_info['sample_data'] = target_data.head(3).to_dict('records') if len(target_data) > 0 else []
        
        for keyword in search_keywords:
            results = []
            for col in target_data.columns:
                if target_data[col].dtype == 'object':
                    try:
                        matches = target_data[target_data[col].astype(str).str.contains(keyword, na=False, case=False)]
                        if len(matches) > 0:
                            results.append({
                                'column': col, 
                                'matches': len(matches), 
                                'sample_values': matches[col].unique()[:3].tolist()
                            })
                    except Exception as e_search: 
                        logger.debug(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒãƒƒã‚°æ¤œç´¢ä¸­ã‚¨ãƒ©ãƒ¼ ({col}, {keyword}): {e_search}")
            debug_info['search_results'][keyword] = results
    return debug_info

def extract_targets_from_file(target_data):
    if target_data is None or target_data.empty: 
        return None, None
    
    debug_info = debug_target_file_processing(target_data)
    search_patterns = [
        ('éƒ¨é–€ã‚³ãƒ¼ãƒ‰', ['å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']), 
        ('éƒ¨é–€å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']),
        ('è¨ºç™‚ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ']), 
        ('ç§‘å', ['ç—…é™¢å…¨ä½“', 'å…¨ä½“', 'ç—…é™¢', 'ç·åˆ'])
    ]
    
    target_row = None
    used_pattern = None
    
    for col_name, keywords in search_patterns:
        if col_name in target_data.columns:
            for keyword in keywords:
                try:
                    mask = target_data[col_name].astype(str).str.contains(keyword, na=False, case=False)
                    matches = target_data[mask]
                    if len(matches) > 0: 
                        target_row = matches.iloc[0]
                        used_pattern = f"{col_name}='{keyword}'"
                        logger.info(f"ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿æ¤œç´¢æˆåŠŸ: {used_pattern}")
                        break
                except Exception as e_pat: 
                    logger.debug(f"ç›®æ¨™å€¤æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ©ãƒ¼ ({col_name}, {keyword}): {e_pat}")
            if target_row is not None: 
                break
    
    if target_row is None:
        logger.warning("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ä½“ã€ã«ç›¸å½“ã™ã‚‹è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, debug_info

    target_days = None
    target_admissions = None
    days_columns = ['å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'åœ¨é™¢æ—¥æ•°ç›®æ¨™', 'ç›®æ¨™åœ¨é™¢æ—¥æ•°', 'å»¶ã¹åœ¨é™¢æ—¥æ•°', 'åœ¨é™¢æ—¥æ•°']
    admission_columns = ['æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'å…¥é™¢æ‚£è€…æ•°ç›®æ¨™', 'ç›®æ¨™å…¥é™¢æ‚£è€…æ•°', 'æ–°å…¥é™¢æ‚£è€…æ•°', 'å…¥é™¢æ‚£è€…æ•°']
    
    for col in days_columns:
        if col in target_data.columns:
            try:
                value = target_row[col]
                if pd.notna(value) and str(value).strip() != '':
                    target_days = float(str(value).replace(',', '').replace('äººæ—¥', '').strip())
                    logger.info(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’å–å¾—: {target_days} (åˆ—: {col})")
                    break
            except (ValueError, TypeError) as e: 
                logger.warning(f"å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
    
    for col in admission_columns:
        if col in target_data.columns:
            try:
                value = target_row[col]
                if pd.notna(value) and str(value).strip() != '':
                    target_admissions = float(str(value).replace(',', '').replace('äºº', '').strip())
                    logger.info(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’å–å¾—: {target_admissions} (åˆ—: {col})")
                    break
            except (ValueError, TypeError) as e: 
                logger.warning(f"æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼ (åˆ—: {col}): {e}")
    
    if (target_days is None or target_admissions is None) and 'ç›®æ¨™å€¤' in target_data.columns:
        try:
            general_target = float(str(target_row['ç›®æ¨™å€¤']).replace(',', '').strip())
            if target_days is None: 
                target_days = general_target
                logger.info(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰å»¶ã¹åœ¨é™¢æ—¥æ•°ç›®æ¨™ã‚’è¨­å®š: {target_days}")
            if target_admissions is None: 
                target_admissions = general_target
                logger.info(f"ä¸€èˆ¬ç›®æ¨™å€¤ã‹ã‚‰æ–°å…¥é™¢æ‚£è€…æ•°ç›®æ¨™ã‚’è¨­å®š: {target_admissions}")
        except (ValueError, TypeError) as e: 
            logger.warning(f"ä¸€èˆ¬ç›®æ¨™å€¤ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    return {
        'target_days': target_days, 
        'target_admissions': target_admissions, 
        'used_pattern': used_pattern, 
        'source_row': target_row.to_dict() if target_row is not None else None
    }, debug_info

def process_data_with_progress(base_file_uploader_obj, new_files_uploader_list, target_file_uploader_obj, progress_bar):
    try:
        start_time_total = time.time()
        st.session_state.performance_metrics = st.session_state.get('performance_metrics', {})
        st.session_state.performance_metrics['data_conversion_time'] = 0

        progress_bar.progress(5, text="1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿æº–å‚™ä¸­...")
        load_start_time = time.time()

        df_raw, processed_files_info = load_files(
            base_file_uploader_obj,
            new_files_uploader_list,
            usecols_excel=EXCEL_USE_COLUMNS,
            dtype_excel=EXCEL_DTYPES
        )
        load_end_time = time.time()
        st.session_state.performance_metrics['data_load_time'] = load_end_time - load_start_time

        successful_reads = 0
        failed_files = []
        if processed_files_info:
            for info in processed_files_info:
                if info['status'] == 'success':
                    successful_reads += 1
                else:
                    failed_files.append(f"{info['name']} ({info['message']})")
            if successful_reads > 0:
                st.success(f"{successful_reads} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚")
            if failed_files:
                st.warning(f"{len(failed_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ:")
                for f_info in failed_files:
                    st.caption(f"- {f_info}")
        elif df_raw.empty:
            st.error("èª­ã¿è¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—ã€‚")
            return False, None, None, None, None

        if df_raw.empty:
            st.error("èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿å†…å®¹ãŒç©ºã§ã™ã€‚")
            return False, None, None, None, None

        progress_bar.progress(20, text="1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿çµåˆä¸­...")

        source_info_cols = ['_source_file_', '_source_type_']
        cols_to_drop_before_dup_check = [col for col in source_info_cols if col in df_raw.columns]
        df_raw_for_dup_check = df_raw.drop(columns=cols_to_drop_before_dup_check, errors='ignore')

        progress_bar.progress(22, text="2. é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
        df_processed_duplicates = efficient_duplicate_check(df_raw_for_dup_check)
        del df_raw_for_dup_check, df_raw
        gc.collect()

        target_data = None
        target_file_debug_info = None
        extracted_targets = None
        if target_file_uploader_obj:
            progress_bar.progress(25, text="ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­...")
            try:
                target_file_uploader_obj.seek(0)
                encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'utf-8-sig']
                target_df_temp = None
                for enc in encodings_to_try:
                    try:
                        target_df_temp = pd.read_csv(target_file_uploader_obj, encoding=enc)
                        logger.info(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’{enc}ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                        target_file_uploader_obj.seek(0)
                        break
                    except UnicodeDecodeError:
                        logger.debug(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è©¦è¡Œå¤±æ•—: {enc}")
                        target_file_uploader_obj.seek(0)
                        continue
                if target_df_temp is None or target_df_temp.empty:
                    st.warning("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆé©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ï¼‰ã€‚")
                else:
                    target_data = target_df_temp
                    extracted_targets, target_file_debug_info = extract_targets_from_file(target_data)
                    st.session_state.target_file_debug_info = target_file_debug_info
                    st.session_state.extracted_targets = extracted_targets
                    st.success("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            except Exception as e_target:
                st.warning(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_target)}")
                logger.error(f"ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e_target}", exc_info=True)
                target_data = None
        else:
            logger.info("ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        progress_bar.progress(28, text="ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†å®Œäº†ã€‚")

        progress_bar.progress(30, text="3. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­...")
        preprocess_start_time = time.time()
        df_final, validation_results = integrated_preprocess_data(df_processed_duplicates, target_data_df=target_data)
        preprocess_end_time = time.time()
        st.session_state.performance_metrics['processing_time'] = preprocess_end_time - preprocess_start_time
        del df_processed_duplicates
        gc.collect()

        if df_final is None or df_final.empty:
            progress_bar.progress(100, text="ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.error("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®çµæœã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            if validation_results and validation_results.get('errors'):
                for err_msg in validation_results.get('errors', []): 
                    st.error(err_msg)
            return False, None, None, None, validation_results

        progress_bar.progress(50, text="4. ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ä¸­...")
        st.session_state.validation_results = validation_results
        if validation_results:
            if validation_results.get("errors"):
                st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ç¶™ç¶šã§ãã¾ã›ã‚“ã€‚")
                for err_msg in validation_results["errors"]: 
                    st.error(err_msg)
                if not validation_results.get("is_valid", True):
                     return False, None, None, None, validation_results
            if validation_results.get("warnings"):
                with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®è­¦å‘Š", expanded=False):
                    for warn_msg in validation_results["warnings"]: 
                        st.warning(warn_msg)

        progress_bar.progress(85, text="5. å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­...")
        all_results = None
        try:
            all_results = generate_filtered_summaries(df_final, None, None)
        except Exception as e_summary:
            st.warning(f"å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_summary}")
            logger.error(f"å…¨ä½“ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e_summary}", exc_info=True)

        if all_results is None or not all_results.get("summary", pd.DataFrame()).empty is False:
            default_latest_date = df_final["æ—¥ä»˜"].max() if not df_final.empty and "æ—¥ä»˜" in df_final.columns else pd.Timestamp.now().normalize()
            all_results = {
                "latest_date": default_latest_date,
                "summary": pd.DataFrame(), 
                "weekday": pd.DataFrame(), 
                "holiday": pd.DataFrame(),
                "monthly_all": pd.DataFrame(), 
                "monthly_weekday": pd.DataFrame(), 
                "monthly_holiday": pd.DataFrame(),
            }
            if df_final is not None and not df_final.empty :
                 st.warning("å…¨ä½“çµæœã®é›†è¨ˆã«ä¸€éƒ¨å¤±æ•—ã—ãŸãŸã‚ã€é™å®šçš„ãªçµæœã«ãªã‚Šã¾ã™ã€‚")

        latest_data_date_obj = all_results.get("latest_date", pd.Timestamp.now().normalize())

        progress_bar.progress(95, text="6. ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®åˆæœŸåŒ–ä¸­...")
        if df_final is not None and not df_final.empty:
            initialize_all_mappings(df_final, target_data)
            logger.info("è¨ºç™‚ç§‘ãŠã‚ˆã³ç—…æ£Ÿã®ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’åˆæœŸåŒ–ãƒ»æ›´æ–°ã—ã¾ã—ãŸã€‚")

        total_time_taken = time.time() - start_time_total
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†å…¨ä½“å®Œäº†ã€‚å‡¦ç†æ™‚é–“: {total_time_taken:.1f}ç§’, ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_final) if df_final is not None else 0}")

        if 'performance_logs' not in st.session_state: 
            st.session_state.performance_logs = []
        st.session_state.performance_logs.append({
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"), 
            'operation': 'ãƒ‡ãƒ¼ã‚¿å‡¦ç†å…¨ä½“', 
            'duration': total_time_taken,
            'details': {
                'rows': len(df_final) if df_final is not None else 0,
                'columns': len(df_final.columns) if df_final is not None and hasattr(df_final, 'columns') else 0,
                'files_new': len(new_files_uploader_list) if new_files_uploader_list else 0,
            }
        })
        progress_bar.progress(100, text=f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å‡¦ç†æ™‚é–“: {total_time_taken:.1f}ç§’")
        return True, df_final, target_data, all_results, latest_data_date_obj

    except Exception as e_main:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_main}", exc_info=True)
        progress_bar.progress(100, text=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_main)}")
        st.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_main)}")
        # ä¿®æ­£ï¼štracebackã‚’é©åˆ‡ã«ä½¿ç”¨
        st.error(traceback.format_exc())
        return False, None, None, None, None


def create_data_processing_tab():
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

    with st.expander("â„¹ï¸ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã«ã¤ã„ã¦", expanded=False):
        st.markdown("""
        **ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®æµã‚Œ:**
        1. **å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«**: ãƒ¡ã‚¤ãƒ³ã¨ãªã‚‹å…¥é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰
        2. **è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«**: è£œå®Œãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€è¤‡æ•°å¯ï¼‰
        3. **ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ«**: éƒ¨é–€åˆ¥ç›®æ¨™è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€CSVå½¢å¼ï¼‰

        **å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ (å…¥é™¢ãƒ‡ãƒ¼ã‚¿):** Excel (.xlsx, .xls)
        **å¿…è¦ãªåˆ—å (æŸ”è»Ÿã«å¯¾å¿œè©¦è¡Œ):**
        ç—…æ£Ÿã‚³ãƒ¼ãƒ‰, è¨ºç™‚ç§‘å, æ—¥ä»˜, åœ¨é™¢æ‚£è€…æ•°, å…¥é™¢æ‚£è€…æ•°, ç·Šæ€¥å…¥é™¢æ‚£è€…æ•°, é€€é™¢æ‚£è€…æ•°, æ­»äº¡æ‚£è€…æ•°
        """)

    if 'data_processing_initialized' not in st.session_state:
        st.session_state.data_processing_initialized = True
        st.session_state.data_processed = False
        st.session_state.df = None
        st.session_state.target_data = None
        st.session_state.all_results = None
        st.session_state.validation_results = None
        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
        st.session_state.target_file_debug_info = None
        st.session_state.extracted_targets = None
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {
                'data_load_time': 0, 
                'data_conversion_time': 0, 
                'processing_time': 0
            }

    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    base_file_key = "dp_base_file_uploader"
    new_files_key = "dp_new_files_uploader"
    target_file_key = "dp_target_file_uploader"

    col_f1_dp, col_f2_dp, col_f3_dp = st.columns(3)
    with col_f1_dp:
        base_file_uploader_widget_dp = st.file_uploader(
            "å›ºå®šãƒ•ã‚¡ã‚¤ãƒ« (Excel)", type=["xlsx", "xls"], key=base_file_key,
            help="ãƒ¡ã‚¤ãƒ³ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã€‚éå»å‡¦ç†æ¸ˆã¿ã®åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨å¯ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸è¦ï¼‰ã€‚"
        )
    with col_f2_dp:
        new_files_uploader_widget_dp = st.file_uploader(
            "è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« (Excel)", type=["xlsx", "xls"], accept_multiple_files=True,
            key=new_files_key, help="è£œå®Œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°å¯ï¼‰ã€‚"
        )
    with col_f3_dp:
        target_file_uploader_widget_dp = st.file_uploader(
            "ç›®æ¨™å€¤ãƒ•ã‚¡ã‚¤ãƒ« (CSV)", type=["csv"], key=target_file_key,
            help="éƒ¨é–€åˆ¥ã®ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVå½¢å¼ï¼‰ã€‚"
        )

    app_data_dir_val_dp = get_app_data_dir()
    parquet_base_path_val_dp = os.path.join(app_data_dir_val_dp, "processed_base_data.parquet") if app_data_dir_val_dp else None
    can_process_now_dp = False
    base_file_info_dp = get_base_file_info(app_data_dir_val_dp)

    if base_file_uploader_widget_dp is not None:
        can_process_now_dp = True
    elif parquet_base_path_val_dp and os.path.exists(parquet_base_path_val_dp) and base_file_info_dp:
        if base_file_uploader_widget_dp is None:
             st.info(f"ä»¥å‰å‡¦ç†ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€Œ{base_file_info_dp.get('file_name', 'ä¸æ˜')}ã€ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚")
        can_process_now_dp = True
    elif new_files_uploader_widget_dp:
        can_process_now_dp = True

    if can_process_now_dp:
        if not st.session_state.get('data_processed', False):
            process_button_key_dp_run = "process_data_button_dp_tab_run"
            if st.button("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œ", key=process_button_key_dp_run, use_container_width=True):
                base_file_to_process_dp = base_file_uploader_widget_dp
                new_files_to_process_dp = new_files_uploader_widget_dp if new_files_uploader_widget_dp else []
                target_file_to_process_dp = target_file_uploader_widget_dp

                progress_bar_ui_main_dp = st.progress(0, text="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                success_flag_dp, df_result_main_dp, target_data_result_main_dp, all_results_main_dp, last_val_or_validation_res = process_data_with_progress(
                    base_file_to_process_dp, new_files_to_process_dp, target_file_to_process_dp, progress_bar_ui_main_dp
                )
                
                if success_flag_dp and df_result_main_dp is not None and not df_result_main_dp.empty:
                    st.session_state.df = df_result_main_dp
                    st.session_state.target_data = target_data_result_main_dp
                    st.session_state.all_results = all_results_main_dp
                    st.session_state.data_processed = True
                    
                    if isinstance(last_val_or_validation_res, pd.Timestamp):
                        st.session_state.latest_data_date_str = last_val_or_validation_res.strftime("%Yå¹´%mæœˆ%dæ—¥")
                    else:
                        st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº† (æ—¥ä»˜ä¸æ˜)"
                        if isinstance(last_val_or_validation_res, dict):
                            st.session_state.validation_results = last_val_or_validation_res

                    st.success(f"ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}")
                    st.session_state.mappings_initialized_after_processing = True
                    perform_cleanup(deep=True)
                    st.rerun()
                else:
                    if not success_flag_dp:
                         st.error("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    if isinstance(last_val_or_validation_res, dict) and last_val_or_validation_res.get('errors'):
                        st.error("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚")
                        for err_msg_dp in last_val_or_validation_res.get('errors', []): 
                            st.error(err_msg_dp)
        else:
            st.success(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ¸ˆã¿ï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {st.session_state.latest_data_date_str}ï¼‰")
            if st.session_state.get('target_data') is not None: 
                st.success("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿æ¸ˆã¿ã§ã™ã€‚")
            else: 
                st.info("ç›®æ¨™å€¤ãƒ‡ãƒ¼ã‚¿ã¯èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

            if st.session_state.get('df') is not None:
                df_display_main_dp_after = st.session_state.df
                with st.expander("ãƒ‡ãƒ¼ã‚¿æ¦‚è¦", expanded=True):
                    col1_sum_dp_after, col2_sum_dp_after, col3_sum_dp_after = st.columns(3)
                    with col1_sum_dp_after:
                        if not df_display_main_dp_after.empty and 'æ—¥ä»˜' in df_display_main_dp_after.columns:
                            min_dt_dp_after = df_display_main_dp_after['æ—¥ä»˜'].min()
                            max_dt_dp_after = df_display_main_dp_after['æ—¥ä»˜'].max()
                            if pd.notna(min_dt_dp_after) and pd.notna(max_dt_dp_after):
                                st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", f"{min_dt_dp_after.strftime('%Y/%m/%d')} - {max_dt_dp_after.strftime('%Y/%m/%d')}")
                            else: 
                                st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "N/A (ç„¡åŠ¹ãªæ—¥ä»˜)")
                        else: 
                            st.metric("ãƒ‡ãƒ¼ã‚¿æœŸé–“", "N/A")
                    with col2_sum_dp_after: 
                        st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{len(df_display_main_dp_after):,}")
                    with col3_sum_dp_after: 
                        st.metric("ç—…æ£Ÿæ•°", f"{df_display_main_dp_after['ç—…æ£Ÿã‚³ãƒ¼ãƒ‰'].nunique() if 'ç—…æ£Ÿã‚³ãƒ¼ãƒ‰' in df_display_main_dp_after.columns else 'N/A'}")

                    col1_sum2_dp_after, col2_sum2_dp_after, col3_sum2_dp_after = st.columns(3)
                    with col1_sum2_dp_after: 
                        st.metric("è¨ºç™‚ç§‘æ•°", f"{df_display_main_dp_after['è¨ºç™‚ç§‘å'].nunique() if 'è¨ºç™‚ç§‘å' in df_display_main_dp_after.columns else 'N/A'}")
                    with col2_sum2_dp_after: 
                        st.metric("å¹³æ—¥æ•°", f"{(df_display_main_dp_after['å¹³æ—¥åˆ¤å®š'] == 'å¹³æ—¥').sum()}" if "å¹³æ—¥åˆ¤å®š" in df_display_main_dp_after.columns else "N/A")
                    with col3_sum2_dp_after: 
                        st.metric("ä¼‘æ—¥æ•°", f"{(df_display_main_dp_after['å¹³æ—¥åˆ¤å®š'] == 'ä¼‘æ—¥').sum()}" if "å¹³æ—¥åˆ¤å®š" in df_display_main_dp_after.columns else "N/A")

                    perf_metrics_disp_dp_after = st.session_state.get('performance_metrics', {})
                    if perf_metrics_disp_dp_after:
                        st.subheader("å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
                        pcol1_dp_after, pcol2_dp_after, pcol3_dp_after, pcol4_dp_after = st.columns(4)
                        with pcol1_dp_after: 
                            st.metric("ãƒ‡ãƒ¼ã‚¿èª­è¾¼æ™‚é–“", f"{perf_metrics_disp_dp_after.get('data_load_time', 0):.1f}ç§’")
                        with pcol2_dp_after: 
                            pass
                        with pcol3_dp_after: 
                            st.metric("ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚é–“", f"{perf_metrics_disp_dp_after.get('processing_time', 0):.1f}ç§’")
                        with pcol4_dp_after:
                            try:
                                mem_info_disp_dp_after = log_memory_usage()
                                if mem_info_disp_dp_after:
                                    st.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨", f"{mem_info_disp_dp_after.get('process_mb', 0):.1f} MB ({mem_info_disp_dp_after.get('process_percent', 0):.1f}%)")
                                else:
                                    st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ä¸å¯")
                            except Exception:
                                st.metric("ãƒ¡ãƒ¢ãƒªæƒ…å ±", "å–å¾—ã‚¨ãƒ©ãƒ¼")

                validation_res_main_dp_after = st.session_state.get('validation_results')
                if validation_res_main_dp_after:
                    if validation_res_main_dp_after.get("warnings") or validation_res_main_dp_after.get("info") or validation_res_main_dp_after.get("errors"):
                        with st.expander("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ", expanded=False):
                            for err_msg_disp_dp_after in validation_res_main_dp_after.get("errors", []): 
                                st.error(err_msg_disp_dp_after)
                            for info_msg_disp_dp_after in validation_res_main_dp_after.get("info", []): 
                                st.info(info_msg_disp_dp_after)
                            for warn_msg_disp_main_dp_after in validation_res_main_dp_after.get("warnings", []): 
                                st.warning(warn_msg_disp_main_dp_after)

            if st.button("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚å‰Šé™¤)", key="reset_data_button_dp_tab_v3_final", use_container_width=True):
                st.session_state.data_processed = False
                st.session_state.df = None
                st.session_state.all_results = None
                st.session_state.target_data = None
                st.session_state.validation_results = None
                st.session_state.latest_data_date_str = "ãƒ‡ãƒ¼ã‚¿èª­è¾¼å‰"
                st.session_state.target_file_debug_info = None
                st.session_state.extracted_targets = None
                st.session_state.performance_metrics = {
                    'data_load_time': 0, 
                    'data_conversion_time': 0, 
                    'processing_time': 0
                }
                st.session_state.dept_mapping = {}
                st.session_state.dept_mapping_initialized = False
                st.session_state.ward_mapping = {}
                st.session_state.ward_mapping_initialized = False
                st.session_state.mappings_initialized_after_processing = False

                if app_data_dir_val_dp:
                    parquet_to_delete_main_dp_after = os.path.join(app_data_dir_val_dp, "processed_base_data.parquet")
                    info_to_delete_main_dp_after = os.path.join(app_data_dir_val_dp, "base_file_info.json")
                    if os.path.exists(parquet_to_delete_main_dp_after):
                        try: 
                            os.remove(parquet_to_delete_main_dp_after)
                            st.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        except Exception as e_del_pq_dp_after: 
                            logger.warning(f"Parquetå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del_pq_dp_after}")
                    if os.path.exists(info_to_delete_main_dp_after):
                        try: 
                            os.remove(info_to_delete_main_dp_after)
                        except Exception as e_del_info_dp_after: 
                            logger.warning(f"Infoãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e_del_info_dp_after}")
                perform_cleanup(deep=True)
                st.rerun()
    else:
        st.info("ã€Œå›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ä»¥å‰å‡¦ç†ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã§ãã‚‹çŠ¶æ…‹ã«ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€Œè¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã€ã®ã¿ã§ã‚‚å‡¦ç†ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")